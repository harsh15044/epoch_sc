{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d28c6cd-38a6-470f-902f-6567e4b8c515",
   "metadata": {},
   "source": [
    "### Task-1\n",
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff170b57-4af8-44d7-9859-bdc3209f82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37191d-6f8c-44b0-9fd2-47a664caa5df",
   "metadata": {},
   "source": [
    "Let's start by implementing a node class for building the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1729c99c-4953-411d-8b29-8076ee565ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__ (self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left #feature<= threshold\n",
    "        self.right = right #feature > threshold\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0db05-e99b-4c6c-9ec3-755312f3d4f7",
   "metadata": {},
   "source": [
    "Let us now implement the `DecisionTreeClassifier` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f2a98a-1019-415c-9f7e-4f67fb1ad50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    A simple implemetation of Decision Tree Classifier.\n",
    "\n",
    "    parameters:\n",
    "        function    : What method to use to calculate impurity. Currently supports \"gini\" and \"entropy\"\n",
    "        max_depth   : Maximum allowed depth of tree\n",
    "        min_samples : minimum number of samples going to a node for it to be a valid decision node.\n",
    "\n",
    "    Methods (for user):\n",
    "        fit()         : to learn the decision tree\n",
    "        predict()     : predicts the label for a list of given points\n",
    "        predict_one() : predicts for a single given point\n",
    "        plot_tree()   : Plots the decision tree\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, function=\"gini\", max_depth=None, min_samples = None):\n",
    "\n",
    "        #checking if valid function provided or not\n",
    "        if(function not in [\"gini\", \"entropy\"]):\n",
    "            raise ValueError('The parameter \"function\" can only take values \"gini\" or \"entropy\".')\n",
    "\n",
    "        #checking if valid max_depth provided or not\n",
    "        if(max_depth != None and (max_depth==0 or not max_depth.is_integer())):\n",
    "            raise ValueError(\"The parameter max_depth needs to be an integer greater than or equal to 1\")\n",
    "\n",
    "        self.function=function\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76882b-0f55-4626-a0b2-c69aaf39b289",
   "metadata": {},
   "source": [
    "Function to calculate the impurity. \n",
    "\n",
    "$$\\text{Gini Impurity} = 1 - \\sum_{i=1}^n p_i^2$$\n",
    "\n",
    "$$\\text{Entropy} = \\sum_{i=1}^n -p_i \\log(p_i)$$\n",
    "\n",
    "Where $p_i$ are the probability of the $n$ different types of classes present in that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d650bbd-8de9-4518-b4fa-707cfb1c2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_impurity(self, y):\n",
    "    impurity = None\n",
    "\n",
    "    #finding the distinct values in y\n",
    "    distinct = set(y)\n",
    "    probabilities = []\n",
    "\n",
    "    #calculating probabilities\n",
    "    for element in distinct:\n",
    "        freq = np.sum(y==element)\n",
    "        probabilities.append(freq/len(y))\n",
    "\n",
    "    #calculating impurity according to provided function\n",
    "    if(self.function==\"gini\"):\n",
    "        impurity = 1\n",
    "        for prob in probabilities:\n",
    "            impurity -= prob**2\n",
    "    else:\n",
    "        impurity = 0\n",
    "        for prob in probabilities:\n",
    "            if(prob>0):\n",
    "                impurity += (-1) * prob * np.log2(prob)\n",
    "\n",
    "    return impurity\n",
    "\n",
    "DecisionTreeClassifier.calculate_impurity = calculate_impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345d9a8-1e9c-41e8-8b2c-9028e3f27bec",
   "metadata": {},
   "source": [
    "This finds the best feature and threshold to split. It goes through every feature with every possible value of threshold and returns the one with minimum weighted children Impurity, where\n",
    "$$\\text{Weighted children Entropy} = p(\\text{left node}) \\cdot \\text{left node impurity} + p(\\text{right node}) \\cdot \\text{right node impurity}$$\n",
    "The probability of a child node is calculated as follows\n",
    "$$p(\\text{child node}) = \\frac{\\text{samples going to this child node}}{\\text{total samples in current node}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "827c049e-4736-4b0a-b9c0-5f3d3131e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(self, X,y):\n",
    "    optimal_entropy = np.inf #max can be inf\n",
    "    optimal_feature = None\n",
    "\n",
    "    #going through each feature\n",
    "    for index in range(X.shape[1]):\n",
    "        #testing each possible split\n",
    "        #first will get the splits by getting the unique elements\n",
    "        unique_elements = set(X[:, index])\n",
    "        unique_elements = list(unique_elements)\n",
    "        unique_elements.sort()\n",
    "\n",
    "        for i in range(len(unique_elements) - 1):\n",
    "            temp_threshold = (unique_elements[i] + unique_elements[i+1])/2\n",
    "\n",
    "            left_values = []\n",
    "            right_values = []\n",
    "\n",
    "            for j in range(len(X)):\n",
    "                if(X[j,index] <= temp_threshold):\n",
    "                    left_values.append(j)\n",
    "                else:\n",
    "                    right_values.append(j)\n",
    "\n",
    "\n",
    "\n",
    "            left_impurity = self.calculate_impurity([y[index] for index in left_values])\n",
    "            right_impurity = self.calculate_impurity([y[index] for index in right_values])\n",
    "\n",
    "            weighted_entropy = (len(left_values) / len(X)) * left_impurity + (len(right_values) / len(X)) * right_impurity\n",
    "\n",
    "            #checking if we got better gain in this case\n",
    "            if(optimal_entropy >= weighted_entropy):\n",
    "                optimal_entropy = weighted_entropy\n",
    "                optimal_feature = (index, temp_threshold)\n",
    "\n",
    "    #by now we have our optimal feature to split across\n",
    "    return optimal_feature[0], optimal_feature[1]\n",
    "\n",
    "DecisionTreeClassifier.find_best_split = find_best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11603f00-60f3-4f26-9d9b-390a60d9579f",
   "metadata": {},
   "source": [
    "fit function, that learns the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83fa0c58-9dab-436f-a790-4ce6c7dc0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    #checking if they have the same size\n",
    "    if(len(X)!=len(y)):\n",
    "        raise ValueError(\"The lengths of X and y do not match.\")\n",
    "\n",
    "    #start building the tree\n",
    "    self.root = self.build_tree(X,y,0)\n",
    "\n",
    "DecisionTreeClassifier.fit = fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318da083-d2ad-469d-a873-1c5fd6cd84c0",
   "metadata": {},
   "source": [
    "Main learning function. First checks if the this Node can be a decision node or not. if not, returns as a leaf node. If it can become a decision node, finds the optimal feature to split on. It splits according to that and calls itself again to learn further from the children Nodes. finally returns a decision node with the feature, threshold and children nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e95257e-7365-4397-a509-0a595826ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(self, X, y, depth):\n",
    "\n",
    "    #taking out all available labels in an array\n",
    "    labels = np.unique(y)\n",
    "\n",
    "    #checking if all labels are same, then return a leaf Node\n",
    "    if(len(labels)==1):\n",
    "        return Node(value=y[0])\n",
    "\n",
    "    #checking if more than max_depth\n",
    "    if(self.max_depth != None and self.max_depth <= max_depth):\n",
    "        count = np.array([sum(y==label) for label in labels])\n",
    "        return Node(value=labels[np.argmax(count)])\n",
    "\n",
    "    #checking if number of samples has fallen below a threshold\n",
    "    if(self.min_samples != None and len(X)<min_samples):\n",
    "        count = np.array([sum(y==label) for label in labels])\n",
    "        return Node(value=labels[np.argmax(count)])\n",
    "\n",
    "    #we can split\n",
    "    #lets find the best features and threshold to split\n",
    "    feature_index, threshold = self.find_best_split(X,y)\n",
    "\n",
    "\n",
    "    #splitting the data accordingly\n",
    "    left_X =[]\n",
    "    left_y =[]\n",
    "    right_X = []\n",
    "    right_y = []\n",
    "    for i in range(len(X)):\n",
    "        if (X[i, feature_index] <= threshold):\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "\n",
    "\n",
    "    left_X = np.array(left_X)\n",
    "    left_y = np.array(left_y)\n",
    "    right_X = np.array(right_X)\n",
    "    right_y = np.array(right_y)\n",
    "\n",
    "    \n",
    "    #making the tree for each of the children of this Node by recusrion\n",
    "    left_node = self.build_tree(left_X, left_y, depth+1)\n",
    "    right_node = self.build_tree(right_X, right_y, depth+1)\n",
    "\n",
    "\n",
    "    #return the built tree (or subtree)\n",
    "    return Node(feature_index = feature_index, threshold = threshold, left=left_node, right= right_node)\n",
    "\n",
    "DecisionTreeClassifier.build_tree = build_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829dd61-ea59-4198-9ccf-a65c1136586b",
   "metadata": {},
   "source": [
    "This function predicts the label for a new data point. It travels the tree until it reaches a leaf node and returns the leaf node value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb0e0bf-eb62-49d7-aeb7-6e1e04fea727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X_test, labels):\n",
    "    predicted_labels=[]\n",
    "    for x in X_test:\n",
    "        predicted_labels.append(labels[self.predict_helper(x, self.root)])\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def predict_one(self, x, labels):\n",
    "    return labels[self.traverse_tree(x, self.root)]\n",
    "\n",
    "\n",
    "def predict_helper(self, x, current_node):\n",
    "\n",
    "    #checking if current node is a leaf or not\n",
    "    if(current_node.value != None):\n",
    "        return current_node.value\n",
    "\n",
    "    #current node is not leaf node\n",
    "    index = current_node.feature_index\n",
    "    threshold = current_node.threshold\n",
    "\n",
    "    if(x[index] <= threshold):\n",
    "        return self.predict_helper(x,current_node.left)\n",
    "    else:\n",
    "        return self.predict_helper(x,current_node.right)\n",
    "\n",
    "DecisionTreeClassifier.predict = predict\n",
    "DecisionTreeClassifier.predict_one = predict_one\n",
    "DecisionTreeClassifier.predict_helper = predict_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb99229-adae-4056-b47b-af9f11c79e39",
   "metadata": {},
   "source": [
    "Now that we are done with the implementation, let's put it into action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f8943f-0141-4d71-a1c2-8123f5418121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    [12.0, 1.5, 1, 'Wine'],\n",
    "    [5.0, 2.0, 0, 'Beer'],\n",
    "    [40.0, 0.0, 1, 'Whiskey'],\n",
    "    [13.5, 1.2, 1, 'Wine'],\n",
    "    [4.5, 1.8, 0, 'Beer'],\n",
    "    [38.0, 0.1, 1, 'Whiskey'],\n",
    "    [11.5, 1.7, 1, 'Wine'],\n",
    "    [5.5, 2.3, 0, 'Beer']\n",
    "]\n",
    "\n",
    "decision_tree_classifer = DecisionTreeClassifier(function=\"entropy\")\n",
    "\n",
    "X, labels_train = np.array([row[:-1] for row in data]), np.array([row[-1] for row in data])\n",
    "\n",
    "#encoding the y labels\n",
    "#Wine = 0, Beer = 1, Whiskey = 2\n",
    "y=[]\n",
    "for i in range(len(labels_train)):\n",
    "    if(labels_train[i]==\"Wine\"):\n",
    "        y.append(0)\n",
    "    elif(labels_train[i]==\"Beer\"):\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(2)\n",
    "\n",
    "labels =[\"Wine\",\"Beer\", \"Whiskey\"]\n",
    "\n",
    "\n",
    "decision_tree_classifer.fit(X,y)\n",
    "\n",
    "#checking if it correctly predicts the training data\n",
    "decision_tree_classifer.predict(X, labels) == labels_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b51ef-0ccc-4a05-bd9b-14b22457792f",
   "metadata": {},
   "source": [
    "Great, it predicts all the training points correctly (it should though). \\\n",
    "Lets try to test it on the `test_data` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c77752f3-55a9-4944-b4b1-bc2b5243ac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beer', 'Whiskey', 'Wine']\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([\n",
    "    [6.0, 2.1, 0],   # Expected: Beer\n",
    "    [39.0, 0.05, 1], # Expected: Whiskey\n",
    "    [13.0, 1.3, 1]   # Expected: Wine\n",
    "])\n",
    "\n",
    "y_predicted = decision_tree_classifer.predict(test_data, labels)\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b483cc1-ea79-448d-b0e5-7745ceee7cbd",
   "metadata": {},
   "source": [
    "Hurray! It works for the test data as well. \\\n",
    "Implemnting the Plot tree function for printing the decision \n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fdcd54b-1a54-48c6-bc0b-d239ec47d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(self, feature_names, labels):\n",
    "    self.traverse_print(self.root, feature_names,labels, append=\"\")\n",
    "\n",
    "def traverse_print(self, node, feature_names,labels ,append=\"\"):\n",
    "    if(node.value!=None):#Leaf node\n",
    "        print(append + f\"It is {labels[node.value]}.\")\n",
    "        return\n",
    "\n",
    "    print(append  + f\"├─If {feature_names[node.feature_index]} <= {node.threshold} : \")\n",
    "    self.traverse_print(node.left, feature_names,labels, append+\"│  \")\n",
    "    print(append  + f\"└─Else ({feature_names[node.feature_index]} > {node.threshold}): \")\n",
    "    self.traverse_print(node.right, feature_names, labels, append+\"  \")\n",
    "\n",
    "\n",
    "DecisionTreeClassifier.plot_tree = plot_tree\n",
    "DecisionTreeClassifier.traverse_print = traverse_print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b81d91-52bc-4ce6-b0e9-9dc4d1a6d55a",
   "metadata": {},
   "source": [
    "Lets test that out and have a look at our decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d01d17-b1cb-48e3-9d34-b4a41e70a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├─If Color <= 0.5 : \n",
      "│  It is Beer.\n",
      "└─Else (Color > 0.5): \n",
      "  ├─If Sugar <= 0.65 : \n",
      "  │  It is Whiskey.\n",
      "  └─Else (Sugar > 0.65): \n",
      "    It is Wine.\n"
     ]
    }
   ],
   "source": [
    "decision_tree_classifer.plot_tree([\"Alcohol Content\", \"Sugar\", \"Color\"], [\"Wine\", \"Beer\", \"Whiskey\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0586daa2-c3f7-438d-9c29-e90f4da51b46",
   "metadata": {},
   "source": [
    "We can clearly see our decision tree. It doesn't consider alcohol content for now because we have a small number of data points. As the number of data points increase, it will start considering alochol content also for deciding (if it affects the decision)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
