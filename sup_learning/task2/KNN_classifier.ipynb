{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66963e69-76f5-4f7e-a5a5-6275731950d6",
   "metadata": {},
   "source": [
    "### Task-2\n",
    "## K-Nearest Neighbors (KNN) Classifier - From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2954a7-486c-47e7-89d3-2f0363567f2d",
   "metadata": {},
   "source": [
    "Let's kick-off by importing modules and writing helper functions\n",
    "\n",
    "For distances, we are using\n",
    "$$\\text{cosine distance} = 1- \\frac{a^\\top b}{||a||\\cdot||b||}$$\n",
    "$$\\text{minkowski distance} = \\left( \\sum_{i=1}^n |a_i-b_i|^p\\right)^{1/p}$$\n",
    "and manhattan distance and euclidean distances are just minkowski distance with $p=1$ and $p=2$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43019368-f689-4ae5-b8e6-11ca87404e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statistics import multimode\n",
    "\n",
    "def calculate_distance(a,b,p):\n",
    "    distance = None\n",
    "    if p==None:\n",
    "        distance = 1 - (a.T@b)/(np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    elif p==np.inf:\n",
    "        distance = np.max(np.abs(a-b))\n",
    "    else:\n",
    "        distance = (np.sum((np.abs(a-b))**p))**(1/p)\n",
    "\n",
    "    return distance\n",
    "\n",
    "def find_mode(numbers):\n",
    "    #getting an array of mode(s)\n",
    "    mode = multimode(numbers)\n",
    "\n",
    "    #Return the mode\n",
    "    #return randmoly if multiple modes found\n",
    "    return np.random.choice(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e1360-df94-4a66-bb1e-afa2f57902b8",
   "metadata": {},
   "source": [
    "Here comes our implementation of KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59f98ab-f8f1-4161-97fd-afd3480d764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classifier():\n",
    "    \"\"\"\n",
    "        A simple implementation of the KNN algorithm.\n",
    "\n",
    "        parameters:\n",
    "            k               : how many neighbours to consider for predicting the label\n",
    "            distance_metric : on what basis to find the nearest neighbors. Currently supports \"euclidean\", \"manhattan\", \"minkowski\", \"cosine\".\n",
    "            p               : p value required for minkowski distance. pass 'np.inf' for L-infinity norm\n",
    "            is_weighted     : Set to True if you want to implement weighted KNN.\n",
    "            normalization   : Whether to normalize the features or not. None for not normalizing. currently supported \"z-score\".\n",
    "\n",
    "        Methods:\n",
    "            train_test_split() : splits the data into train and test data according to the parameter train_data (which tells what proportion of the total data to be taken for trainig). Returns 4 arrays.\n",
    "            fit()              : Stores the input data to predict label for new points\n",
    "            predict()          : predicts the label for a set of given points\n",
    "            predict_one()      : predicts the label for a single given point\n",
    "            accuracy_checker() : Compares the predicted label with the actual labels to calculate the accuracy\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,k=3, distance_metric=\"euclidean\", p=None, is_weighted=False, normalization=None):\n",
    "\n",
    "        #checking if invalid parameter passed for distance_metric\n",
    "        if(distance_metric not in [\"euclidean\", \"manhattan\", \"minkowski\", \"cosine\"]):\n",
    "            raise ValueError('distance_metric can only take values \"euclidean\", \"manhattan\", \"minkowski\", \"cosine\"')\n",
    "\n",
    "        #checking if valid p is supplied in case of minkowski\n",
    "        if(distance_metric == \"minkowski\" and (p==None or p<1)):\n",
    "            raise ValueError(\"The value of p should be a nunmerical value greater than or equal to 1\")\n",
    "\n",
    "        #everything's fine, let's proceed\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.is_weighted = is_weighted\n",
    "\n",
    "        #We will get the data when the fit function is called\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        #assigning self.p\n",
    "        if(distance_metric==\"euclidean\"):\n",
    "            self.p=2\n",
    "        elif(distance_metric==\"manhattan\"):\n",
    "            self.p=1\n",
    "        elif(distance_metric==\"minkowski\"):\n",
    "            self.p=p\n",
    "        elif(distance_metric==\"cosine\"):\n",
    "            self.p=None\n",
    "\n",
    "        self.normalization = normalization\n",
    "        self.mean_features= None\n",
    "        self.std_deviation_features = None\n",
    "\n",
    "        #done with the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536016f7-6cdc-4d2c-8ec9-2d349bc7c282",
   "metadata": {},
   "source": [
    "Implementing the train_test_split to split data into training and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626cc336-fd98-40f3-8ff1-ce54df8a2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(self, X, y, train_data =0.7):\n",
    "    #checking if dimensions of X and y match\n",
    "    if(len(X) != len(y)):\n",
    "        raise SizeError(f\"The size of X ({len(X)}) does not match with that of y ({len(y)})\")\n",
    "\n",
    "    #checking if valid train_data parameter passed or not\n",
    "    if(train_data<=0 or train_data >1):\n",
    "        raise ValueError(f\"Invalid value for train_data. It should belong to (0,1]\")\n",
    "\n",
    "    #shuffling the indices\n",
    "    indices = range(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test  = []\n",
    "    y_test  =[]\n",
    "\n",
    "    #taking the first train_data proportion of indoces for training and later for test\n",
    "    num_training_points = len(X)//train_data\n",
    "    for i in range(num_training_points+1):\n",
    "        X_train.append(X[i])\n",
    "        y_train.append(y[i])\n",
    "\n",
    "    for i in range(num_training_points+1, len(X)):\n",
    "        X_test.append(X[i])\n",
    "        y_test.append(y[i])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "KNN_Classifier.train_test_split = train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed9fac-7bd2-46bf-ae86-72e82c81257e",
   "metadata": {},
   "source": [
    "Learning the model (simply storing the data points for KNN), with normalization(if any)\n",
    "$$ z = \\frac{x - \\mu}{\\sigma}$$\n",
    "where $\\mu$ is the mean and $\\sigma$ is the std. devation of all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a5d277-661d-4ad9-87ba-69d8594dc113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    #checking if dimensions of X and y match\n",
    "    if(len(X) != len(y)):\n",
    "        raise ValueError(f\"The size of X ({len(X)}) does not match with that of y ({len(y)})\")\n",
    "\n",
    "    #converting X to numpy array\n",
    "    for i in range(len(X)):\n",
    "        X[i] = np.array(X[i])\n",
    "    X = np.array(X)\n",
    "    print(len(X.T))\n",
    "\n",
    "    #if normalization asked, normalizaing\n",
    "    if(self.normalization == \"z-score\"):\n",
    "        X_transposed = X.T\n",
    "\n",
    "        self.mean_features = []\n",
    "        self.std_deviation_features = []\n",
    "        for i in range(len(X_transposed)):\n",
    "            self.mean_features.append(np.mean(X_transposed[i]))\n",
    "            self.std_deviation_features.append(np.std(X_transposed[i]))\n",
    "\n",
    "        #converting to numpy array\n",
    "        self.mean_features = np.array(self.mean_features)\n",
    "        self.std_deviation_features = np.array(self.std_deviation_features)\n",
    "\n",
    "        #normalizing each point\n",
    "        X = (X-self.mean_features)/ self.std_deviation_features\n",
    "    \n",
    "    #everything is fine, let's store the data\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "KNN_Classifier.fit = fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4f8d1-a1d9-43e9-b262-1a8765eca8dc",
   "metadata": {},
   "source": [
    "Function to predict the label for a point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd433192-478c-40fc-9212-6485d1628187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(self,x):\n",
    "    #normalizing point if we are doing normalization\n",
    "    if(self.normalization==\"z-score\"):\n",
    "        x = (x - self.mean_features) / self.std_deviation_features\n",
    "\n",
    "    #now calculating the distances\n",
    "    distances = []\n",
    "    for x_data in self.X:\n",
    "        distances.append(calculate_distance(x, x_data, self.p))\n",
    "\n",
    "    #now we have distances array\n",
    "    distances = np.array(distances)\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    k_nearest_indices = sorted_indices[:self.k]\n",
    "\n",
    "    #if data point exists in X, return the same label\n",
    "    if(distances[sorted_indices[0]] == 0):\n",
    "        return (self.y[sorted_indices[0]])\n",
    "\n",
    "    #else we will predict\n",
    "    else:\n",
    "        #if weighted\n",
    "        if(self.is_weighted):\n",
    "            freq = np.zeros(3)\n",
    "\n",
    "            distances_sum = sum([distances[index] for index in k_nearest_indices])\n",
    "            for index in k_nearest_indices:\n",
    "                freq[y[index]] += 1/distances[index]\n",
    "\n",
    "            return np.argmax(freq)\n",
    "\n",
    "        #normal KNN\n",
    "        else:\n",
    "            labels = [self.y[index] for index in k_nearest_indices]\n",
    "            return find_mode(labels)\n",
    "\n",
    "KNN_Classifier.predict_one = predict_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3907b5-d40f-46e5-b234-5091edb0cc3e",
   "metadata": {},
   "source": [
    "This function predicts for a list of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fafbeae8-e900-4aee-94ed-8ad9c85e5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X, labels=None):\n",
    "    predicted = []\n",
    "\n",
    "    for x in X:\n",
    "        predicted.append(self.predict_one(x))\n",
    "\n",
    "    return predicted\n",
    "\n",
    "KNN_Classifier.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33fd52-700b-4d67-9e4b-779345ff6a93",
   "metadata": {},
   "source": [
    "Implementing a basic accuracy checker as asked. We know\n",
    "$$\\text{accuracy} = \\frac{\\text{Correctly predicted samples}}{\\text{Total samples}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06d61b0-279e-4a09-8e56-2fa0430cf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_checker(self, y_predicted, y_true):\n",
    "\n",
    "    if(len(y_predicted) != len(y_true)):\n",
    "        raise SizeError(\"Size of given arrays do not match.\")\n",
    "\n",
    "\n",
    "    total = len(y_predicted)\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(len(y_predicted)):\n",
    "        if(y_predicted[i]==y_true[i]):\n",
    "            correct+=1\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "KNN_Classifier.accuracy_checker = accuracy_checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3784b71-c55a-4637-b1c6-07db4a175c0f",
   "metadata": {},
   "source": [
    "Function to print the predicted output with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ddcbb19-e517-4404-9dbd-64d4b49d3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(self, predictions, labels):\n",
    "    modified = []\n",
    "    for i in range(len(predictions)):\n",
    "        modified.append(labels[predictions[i]])\n",
    "\n",
    "    print(modified)\n",
    "\n",
    "KNN_Classifier.print_output = print_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7fc1a-077b-4fa8-a5b0-c78a96e85ec8",
   "metadata": {},
   "source": [
    "Lets use this implementation, and test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cdf1323-7732-44e0-ba24-07efd9f10ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['Banana', 'Apple', 'Orange']\n",
      "The accuracy in this case is 100.0%.\n"
     ]
    }
   ],
   "source": [
    "### given data\n",
    "data = [\n",
    "    [150, 7.0, 1, 'Apple'],\n",
    "    [120, 6.5, 0, 'Banana'],\n",
    "    [180, 7.5, 2, 'Orange'],\n",
    "    [155, 7.2, 1, 'Apple'],\n",
    "    [110, 6.0, 0, 'Banana'],\n",
    "    [190, 7.8, 2, 'Orange'],\n",
    "    [145, 7.1, 1, 'Apple'],\n",
    "    [115, 6.3, 0, 'Banana']\n",
    "]\n",
    "\n",
    "#creating an instance of our class\n",
    "knn_classifier = KNN_Classifier(k=4, is_weighted=True, normalization=\"z-score\")\n",
    "\n",
    "#splitting into features and labels\n",
    "X, labels_train = np.array([row[:-1] for row in data]), np.array([row[-1] for row in data])\n",
    "\n",
    "#encoding the y labels\n",
    "#apples = 0, Banana = 1, Orange = 2\n",
    "y=[]\n",
    "for i in range(len(labels_train)):\n",
    "    if(labels_train[i]==\"Apple\"):\n",
    "        y.append(0)\n",
    "    elif(labels_train[i]==\"Banana\"):\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(2)\n",
    "\n",
    "#fitting our model\n",
    "knn_classifier.fit(X,y)\n",
    "\n",
    "#labels \n",
    "labels = [\"Apple\", \"Banana\", \"Orange\"]\n",
    "\n",
    "#test data\n",
    "test_data = np.array([\n",
    "    [118, 6.2, 0],  # Expected: Banana\n",
    "    [160, 7.3, 1],  # Expected: Apple\n",
    "    [185, 7.7, 2]   # Expected: Orange\n",
    "])\n",
    "y_test = np.array([1,0,2])\n",
    "\n",
    "X_test = np.array(test_data)\n",
    "\n",
    "y_predicted = knn_classifier.predict(X_test)\n",
    "knn_classifier.print_output(y_predicted, labels)\n",
    "\n",
    "print(f\"The accuracy in this case is {knn_classifier.accuracy_checker(y_predicted, y_test) * 100}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6fccaa-1e98-4913-a516-a4188440c5fb",
   "metadata": {},
   "source": [
    "The output is Banana, Apple, Orange, which is what we expected. This implies that our implementation is working fine.\n",
    "\n",
    "On playing around with $k$ values, we realize that only values till $3$ work fine with KNN, however Weighted-KNN works fine even for higher values of $k$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
